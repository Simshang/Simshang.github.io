<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>Gradient Descent | 简说</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="在SVM and Softmax那篇文章中, 我们知道模型最主要的两个模块, 评分函数和代价函数, 我们通过代价函数来表征真实值与模型预测值之间的差异, 当然我们只知道和真实值相差多少是不够的, 我们还要不断的更改模型参数$W$, 使得差异越来越小, 这个过程就是最优化, 这篇文章介绍最优化方法中广泛使用的梯度下降法">
<meta name="keywords" content="梯度下降法">
<meta property="og:type" content="article">
<meta property="og:title" content="Gradient Descent">
<meta property="og:url" content="http://simtalk.cn/2016/09/05/Gradient-Descent/index.html">
<meta property="og:site_name" content="简说">
<meta property="og:description" content="在SVM and Softmax那篇文章中, 我们知道模型最主要的两个模块, 评分函数和代价函数, 我们通过代价函数来表征真实值与模型预测值之间的差异, 当然我们只知道和真实值相差多少是不够的, 我们还要不断的更改模型参数$W$, 使得差异越来越小, 这个过程就是最优化, 这篇文章介绍最优化方法中广泛使用的梯度下降法">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://simtalk.cn/img/Gradient-Descent/svm1d.png">
<meta property="og:image" content="http://simtalk.cn/img/Gradient-Descent/svm_one.jpg">
<meta property="og:image" content="http://simtalk.cn/img/Gradient-Descent/svm_all.jpg">
<meta property="og:image" content="http://simtalk.cn/img/Gradient-Descent/svmbowl.png">
<meta property="og:image" content="http://simtalk.cn/img/Gradient-Descent/dataflow.jpeg">
<meta property="og:updated_time" content="2018-06-02T05:28:22.947Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Gradient Descent">
<meta name="twitter:description" content="在SVM and Softmax那篇文章中, 我们知道模型最主要的两个模块, 评分函数和代价函数, 我们通过代价函数来表征真实值与模型预测值之间的差异, 当然我们只知道和真实值相差多少是不够的, 我们还要不断的更改模型参数$W$, 使得差异越来越小, 这个过程就是最优化, 这篇文章介绍最优化方法中广泛使用的梯度下降法">
<meta name="twitter:image" content="http://simtalk.cn/img/Gradient-Descent/svm1d.png">
  
    <link rel="alternative" href="/atom.xml" title="简说" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="http://7xqkff.com1.z0.glb.clouddn.com/AIer.png" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">simshang</a></h1>
		</hgroup>

		
		<p class="header-subtitle">英泰勒吉斯就一定要实现</p>
		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
						<li>关于我</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
							<li><a href="/categories/life">生活</a></li>
				        
							<li><a href="/categories/Ukelele">音乐</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/Simshang" title="github">github</a>
					        
								<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/shangyan" title="zhihu">zhihu</a>
					        
								<a class="mail" target="_blank" href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=l_T-9vnwue72_dfx_O-69v77ufT4_g" title="mail">mail</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/proto/" style="font-size: 10px;">.proto</a> <a href="/tags/3dConv/" style="font-size: 10px;">3dConv</a> <a href="/tags/AlexNet/" style="font-size: 10px;">AlexNet</a> <a href="/tags/BN/" style="font-size: 10px;">BN</a> <a href="/tags/BRIEF/" style="font-size: 10px;">BRIEF</a> <a href="/tags/BigO/" style="font-size: 10px;">BigO</a> <a href="/tags/Blobs/" style="font-size: 10px;">Blobs</a> <a href="/tags/BoW/" style="font-size: 10px;">BoW</a> <a href="/tags/C/" style="font-size: 14px;">C++</a> <a href="/tags/CDC/" style="font-size: 10px;">CDC</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/Caffe/" style="font-size: 18px;">Caffe</a> <a href="/tags/Container/" style="font-size: 10px;">Container</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Dockerhub/" style="font-size: 10px;">Dockerhub</a> <a href="/tags/Dropout/" style="font-size: 10px;">Dropout</a> <a href="/tags/FCN/" style="font-size: 12px;">FCN</a> <a href="/tags/FTP/" style="font-size: 10px;">FTP</a> <a href="/tags/GBD/" style="font-size: 10px;">GBD</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Github/" style="font-size: 10px;">Github</a> <a href="/tags/GoogLeNet/" style="font-size: 10px;">GoogLeNet</a> <a href="/tags/Harris/" style="font-size: 10px;">Harris</a> <a href="/tags/Hexo/" style="font-size: 14px;">Hexo</a> <a href="/tags/IDE/" style="font-size: 10px;">IDE</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/LaTeX/" style="font-size: 10px;">LaTeX</a> <a href="/tags/Layers/" style="font-size: 12px;">Layers</a> <a href="/tags/Linux/" style="font-size: 12px;">Linux</a> <a href="/tags/Make/" style="font-size: 10px;">Make</a> <a href="/tags/Markdown/" style="font-size: 10px;">Markdown</a> <a href="/tags/Mysql/" style="font-size: 16px;">Mysql</a> <a href="/tags/NIN/" style="font-size: 10px;">NIN</a> <a href="/tags/Nets/" style="font-size: 10px;">Nets</a> <a href="/tags/ORB/" style="font-size: 10px;">ORB</a> <a href="/tags/OS/" style="font-size: 12px;">OS</a> <a href="/tags/Paddle/" style="font-size: 12px;">Paddle</a> <a href="/tags/PyCaffe/" style="font-size: 10px;">PyCaffe</a> <a href="/tags/Python/" style="font-size: 12px;">Python</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/SIFT/" style="font-size: 10px;">SIFT</a> <a href="/tags/SURF/" style="font-size: 10px;">SURF</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Shell/" style="font-size: 10px;">Shell</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Staple/" style="font-size: 10px;">Staple</a> <a href="/tags/TensorFlow/" style="font-size: 10px;">TensorFlow</a> <a href="/tags/UML/" style="font-size: 10px;">UML</a> <a href="/tags/VGG/" style="font-size: 10px;">VGG</a> <a href="/tags/Vim/" style="font-size: 10px;">Vim</a> <a href="/tags/kNN/" style="font-size: 10px;">kNN</a> <a href="/tags/内存/" style="font-size: 10px;">内存</a> <a href="/tags/单元测试/" style="font-size: 10px;">单元测试</a> <a href="/tags/反向传播算法/" style="font-size: 10px;">反向传播算法</a> <a href="/tags/图像增强/" style="font-size: 10px;">图像增强</a> <a href="/tags/图说/" style="font-size: 20px;">图说</a> <a href="/tags/工厂模式/" style="font-size: 10px;">工厂模式</a> <a href="/tags/摇滚/" style="font-size: 14px;">摇滚</a> <a href="/tags/文本分类/" style="font-size: 10px;">文本分类</a> <a href="/tags/最小二乘法/" style="font-size: 10px;">最小二乘法</a> <a href="/tags/梯度下降法/" style="font-size: 14px;">梯度下降法</a> <a href="/tags/模型优化/" style="font-size: 12px;">模型优化</a> <a href="/tags/正则化/" style="font-size: 12px;">正则化</a> <a href="/tags/激活函数/" style="font-size: 10px;">激活函数</a> <a href="/tags/电影/" style="font-size: 10px;">电影</a> <a href="/tags/神经网络/" style="font-size: 12px;">神经网络</a> <a href="/tags/算法/" style="font-size: 10px;">算法</a> <a href="/tags/线性模型/" style="font-size: 12px;">线性模型</a> <a href="/tags/设计模式/" style="font-size: 10px;">设计模式</a> <a href="/tags/随笔/" style="font-size: 12px;">随笔</a> <a href="/tags/面向对象/" style="font-size: 10px;">面向对象</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">北邮在读，计算机视觉与深度学习，喜欢摇滚乐，爱打篮球，极简主义。</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">simshang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="http://7xqkff.com1.z0.glb.clouddn.com/AIer.png" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">simshang</h1>
			</hgroup>
			
			<p class="header-subtitle">英泰勒吉斯就一定要实现</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/categories/life">生活</a></li>
		        
					<li><a href="/categories/Ukelele">音乐</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/Simshang" title="github">github</a>
			        
						<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/shangyan" title="zhihu">zhihu</a>
			        
						<a class="mail" target="_blank" href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=l_T-9vnwue72_dfx_O-69v77ufT4_g" title="mail">mail</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap"><article id="post-Gradient-Descent" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/05/Gradient-Descent/" class="article-date">
  	<time datetime="2016-09-05T14:46:04.000Z" itemprop="datePublished">2016-09-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Gradient Descent
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/梯度下降法/">梯度下降法</a></li></ul>
	</div>

        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/cs231n/">cs231n</a>
	</div>


        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <div id="toc" class="toc-article">
            <strong class="toc-title">文章目录</strong>
            <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#损失函数的可视化分析"><span class="toc-text">损失函数的可视化分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最优化策略"><span class="toc-text">最优化策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度的计算"><span class="toc-text">梯度的计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度下降"><span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型数据流"><span class="toc-text">模型数据流</span></a></li></ol>
        </div>
        
        <p>在<code>SVM and Softmax</code>那篇文章中, 我们知道模型最主要的两个模块, <code>评分函数</code>和<code>代价函数</code>, 我们通过<code>代价函数</code>来表征真实值与模型预测值之间的差异, 当然我们只知道和真实值相差多少是不够的, 我们还要不断的更改模型参数$W$, 使得差异越来越小, 这个过程就是最优化, 这篇文章介绍最优化方法中广泛使用的<code>梯度下降法</code> </p>
<a id="more"></a>
<blockquote>
<p>预备知识请看, <a href="http://simtalk.cn/2016/08/23/%E4%BB%8E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%88%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">从线性模型到神经网络</a> 中的<code>梯度下降法</code>部分</p>
</blockquote>
<h3 id="损失函数的可视化分析"><a href="#损失函数的可视化分析" class="headerlink" title="损失函数的可视化分析"></a><strong>损失函数的可视化分析</strong></h3><p>在最优化方法中, 我们优化的目标函数是<code>代价函数</code>, 优化的变量是参数集$W$, 根据微分求极值的思想, 如果我们可以可视化代价函数的<code>单调性</code>, 就可以很容易发现代价函数的最小值在哪里, 但是参数集$W$通常处于高位空间中, 那么只能在参数集中的一个维度或者两个维度方向上做切面, 我们可以随机生成一个$W$, 将其中的一个参数(一维)或者两个参数(二维)作为变量, 其余的参数固定不变, 那么就可以可视化损失函数了, 如图所示</p>
<p><img src="/img/Gradient-Descent/svm1d.png" alt=""></p>
<ul>
<li><code>一维可视化</code> : 首先我们随机生成一个参数集$W$, 为了使得$W$在某一个方向上变化, 我们随机选定一个方向$W_1$, 通过改变$a$的值$ W+aW_1 $使得参数集在某一维度上变化, 这样就可以看到损失函数$L(W + a W_1)$的变化曲线</li>
</ul>
<p><img src="/img/Gradient-Descent/svm_one.jpg" alt=""></p>
<p><code>PS:</code> 蓝色部分是低损失值区域，红色部分是高损失值区域, 单一样本</p>
<ul>
<li><code>二维可视化</code>: 同样的道理, 我们随机生成一个参数集$W$, 为了使得$W$在某两个方向上变化, 我们随机选定两个方向$W_1,W_2$, 通过$ W+aW_1+bW_2 $使得参数集在某两个维度上变化, 这样就可以看到损失函数$L(W + a W_1+bW_2)$的变化曲线</li>
</ul>
<p><img src="/img/Gradient-Descent/svm_all.jpg" alt=""></p>
<ul>
<li>上图表示的是多个样本的损失值的总体平均值, 因为损失函数的分段线性结构, 所以上图是很多的分段线性结构的平均值</li>
</ul>
<blockquote>
<p>什么是损失函数的分段线性结构?</p>
</blockquote>
<p>$$ L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + 1) \right] $$</p>
<p>通过上式可知, 每个样本的数据损失值是以参数集$W$的线性函数的总和, 举个例子</p>
<ul>
<li>有3个特征维度为一维的样本点$\{ (x_0),(x_1),(x_2) \}$, 数据集有3个类别, 那么无正则项的SVM损失值计算过程如下:</li>
</ul>
<p>$$<br>\begin{align}<br>L_0 = &amp; \max(0, w_1^Tx_0 - w_0^Tx_0 + 1) + \max(0, w_2^Tx_0 - w_0^Tx_0 + 1) \\<br>L_1 = &amp; \max(0, w_0^Tx_1 - w_1^Tx_1 + 1) + \max(0, w_2^Tx_1 - w_1^Tx_1 + 1) \\<br>L_2 = &amp; \max(0, w_0^Tx_2 - w_2^Tx_2 + 1) + \max(0, w_1^Tx_2 - w_2^Tx_2 + 1) \\<br>L = &amp; (L_0 + L_1 + L_2)/3<br>\end{align}<br>$$</p>
<p>由于是一维的,$x_i$和$w_j$都是单个值, 不是向量, 由于$max$函数是分段的, 且与0比较的是线性函数$wx+1$的形式, 如下图所示,</p>
<p><img src="/img/Gradient-Descent/svmbowl.png" alt=""></p>
<ul>
<li>上图是从一个维度的方向上进行可视化的损失函数, x轴代表某个$w_j$, y轴是损失值, 代价函数中的数据损失部分是由多个分段线性损失组合(本质上就是不同的HingeLoss函数)而成, 完整的SVM代价函数的数据损失部分就是将这个形状扩展到高维空间 </li>
</ul>
<p><code>PS</code>: </p>
<ol>
<li><p>根据SVM的损失函数的形状可以判定是一个凸函数, 但是我们将评分函数$f(x)$变为神经网络那么目标函数就不是凸函数了, 而是类似凹凸不平的复杂地形的形状</p>
</li>
<li><p>由于在梯度下降法中有微分运算, $max$函数在拐点处是不可导的, 梯度没有定义, 我们用<code>次梯度</code>的概念来代替<code>梯度</code>的概念</p>
</li>
</ol>
<h3 id="最优化策略"><a href="#最优化策略" class="headerlink" title="最优化策略"></a><strong>最优化策略</strong></h3><p>既然代价函数可以评价某个参数集$W$的好坏程度, 那么我们最优化的目标就是找到能够最小化损失函数的参数集$W$</p>
<ol>
<li><p><strong>初始化参数集</strong> : 随机搜索, 即将$W$随机赋值</p>
</li>
<li><p><strong>迭代优化</strong> : 在随机参数的基础上进行模型参数集的迭代更新, 使得模型的损失值更低</p>
</li>
</ol>
<p><strong>更新参数策略</strong> : </p>
<p>当满足什么条件时, 我们才进行参数的更新, 然后如何更新呢?</p>
<ol>
<li><p>随机本地搜索 : 我们在随机参数集$W$的基础上, 加上一个随机的扰动$\delta W$然后新的参数集$W + \delta W$如果可以使得损失函数值更小, 那么就更新参数, 否则不更新参数</p>
</li>
<li><p>在<code>随机本地搜索</code>中, 通过随机项我们试图找到一个可以使得损失函数下降的方向, 其实不用随机寻找, 在数学上<code>梯度</code>的定义就是函数$f(x)$增长最快的方向(最陡峭), 我们可以通过计算损失函数的梯度来找到下降最快的方向(最陡峭)</p>
</li>
</ol>
<p>在一维函数中，斜率是函数在某一点的瞬时变化率。梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量。一维空间的斜率如下</p>
<p>$$\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}$$</p>
<p><code>PS</code> : 在高维空间中, 梯度是各个维度的斜率组成的向量, 函数有多个参数的时候，在某个方向上的导数为偏导数</p>
<h3 id="梯度的计算"><a href="#梯度的计算" class="headerlink" title="梯度的计算"></a><strong>梯度的计算</strong></h3><p><strong>数值梯度法 : 计算速度慢且不精确</strong></p>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_numerical_gradient</span><span class="params">(f, x)</span>:</span></span><br><span class="line">  <span class="string">""" </span></span><br><span class="line"><span class="string">  a naive implementation of numerical gradient of f at x </span></span><br><span class="line"><span class="string">  - f should be a function that takes a single argument</span></span><br><span class="line"><span class="string">  - x is the point (numpy array) to evaluate the gradient at</span></span><br><span class="line"><span class="string">  """</span> </span><br><span class="line"></span><br><span class="line">  fx = f(x) <span class="comment"># evaluate function value at original point</span></span><br><span class="line">  grad = np.zeros(x.shape)</span><br><span class="line">  h = <span class="number">0.00001</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># iterate over all indexes in x</span></span><br><span class="line">  it = np.nditer(x, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</span><br><span class="line">  <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># evaluate function at x+h</span></span><br><span class="line">    ix = it.multi_index</span><br><span class="line">    old_value = x[ix]</span><br><span class="line">    x[ix] = old_value + h <span class="comment"># increment by h</span></span><br><span class="line">    fxh = f(x) <span class="comment"># evalute f(x + h)</span></span><br><span class="line">    x[ix] = old_value <span class="comment"># restore to previous value (very important!)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the partial derivative</span></span><br><span class="line">    grad[ix] = (fxh - fx) / h <span class="comment"># the slope</span></span><br><span class="line">    it.iternext() <span class="comment"># step to next dimension</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
</code></pre><p>   <strong>注意</strong>: </p>
<ul>
<li>在数学公式中，h的取值是趋近于0的，然而在实际中，用一个很小的数值（比如例子中的1e-5）就足够了。</li>
<li>实际中用中心差值公式（centered difference formula）: $\frac{df(x)}{dx} =\frac{[f(x+h) - f(x-h)]}{ 2 h}$</li>
<li><strong>在梯度负方向上更新：在上面的代码中，为了计算$W_{new}$，要注意我们是向着梯度$df$的负方向去更新，这是因为我们希望损失函数值是降低而不是升高。</strong></li>
<li><code>学习率</code>的影响, 参见<a href="http://simtalk.cn/2016/08/23/%E4%BB%8E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%88%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/#梯度下降法">从线性模型到神经网络</a> 中的<code>梯度下降法</code>部分</li>
<li>效率问题: 计算数值梯度的复杂性和参数的量线性相关, 在模型中有30730个参数，所以损失函数每走一步就需要计算30731次损失函数的梯度, 神经网络很容易就有上千万的参数，显然这个策略不适合大规模数据，我们需要更好的策略。</li>
</ul>
<ol start="2">
<li><strong>微分分析计算梯度 : 速度快且精确</strong></li>
</ol>
<p>使用有限差值近似计算梯度比较简单，但缺点是近似计算（对于h = 0.00001是选取了一个很小的数值，但真正的梯度定义中h趋向0的极限），且耗费计算资源太多, 但是利用微分来分析，能得到计算梯度的公式（不是近似），用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错。</p>
<p>我们拿$x_i$点来对SVM损失函数来进行举例:</p>
<p>$$ L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right] $$</p>
<p>对$w_{y_i}$进行微分得到,</p>
<p>$$\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) \right) x_i$$</p>
<ul>
<li>如果括号中的条件为真，那么函数值为1，如果为假，则函数值为0。</li>
<li>代码实现的时候比较简单：只需要计算没有满足边界值的分类的数量（由于对损失函数产生了贡献），然后乘以x_i就是梯度了。</li>
</ul>
<p><strong>注意</strong>, 上个式子中是对$w_{y_i}$进行求导, 得到的是对正确分类所对应的参数集$W$的行进行求梯度, 其余的行$(j \neq y_i)$需要对$w_j$求导, 他们的梯度为</p>
<p>$$\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i$$</p>
<ul>
<li>可以将每一行理解为一个线性函数, 我们要优化这个线性函数的参数向量</li>
</ul>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a><strong>梯度下降</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data, weights)</span><br><span class="line">  weights += - step_size * weights_grad </span><br><span class="line">   perform parameter update</span><br></pre></td></tr></table></figure>
<ul>
<li>到目前为止，梯度下降是对神经网络的损失函数最优化中最常用的方法。</li>
</ul>
<p><strong>小批量数据梯度下降（Mini-batch gradient descent）：</strong></p>
<p>在大规模数据的训练中, 训练数据可达百万数量级, 这样更新参数太笨重了, 常用的方法就是利用训练集中的一小部分数据来更新一个参数, 这样做基于一个假设: <code>数据具有相关性</code>, 实际情况中，数据集肯定不会包含重复图像，那么小批量数据的梯度就是对整个数据集梯度的一个近似。因此，在实践中通过计算小批量数据的梯度可以实现更快速地收敛，并以此来进行更频繁的参数更新。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Vanilla Minibatch Gradient Descent</span><br><span class="line">while True:</span><br><span class="line">  data_batch = sample_training_data(data, 256) # sample 256 examples</span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)</span><br><span class="line">  weights += - step_size * weights_grad </span><br><span class="line">  # perform parameter update</span><br></pre></td></tr></table></figure>
<ul>
<li><p>小批量数据策略有个极端情况，那就是每个批量中只有1个数据样本，这种策略被称为<code>随机梯度下降（Stochastic Gradient Descent 简称SGD）</code>, SGD在技术上是指每次使用1个数据来计算梯度，实际上通常使用SGD来指代小批量数据梯度下降</p>
</li>
<li><p>小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参, 它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如32，64，128等, 之所以使用2的指数，是因为在实际中许多向量化操作实现的时候，如果输入数据量是2的倍数，那么运算更快。</p>
</li>
</ul>
<h3 id="模型数据流"><a href="#模型数据流" class="headerlink" title="模型数据流"></a><strong>模型数据流</strong></h3><p><img src="/img/Gradient-Descent/dataflow.jpeg" alt=""></p>
<ul>
<li><p>将参数集$W$和样本$x_i$输入<code>评分函数</code>$f(x_i,W)$得到评分(class scores)</p>
</li>
<li><p>$W$计算得到<code>正则损失(regularization loss)</code>, 真实值$y_i$和<code>评分(class scores)</code>通过相应的损失函数计算得到<code>数据损失(data loss)</code></p>
</li>
<li><p><code>模型总代价</code>$L$ = <code>正则损失</code> + <code>数据损失</code></p>
</li>
</ul>
<blockquote>
<p><strong>特别鸣谢</strong> : 文中部分内容引自知乎专栏 <a href="https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit" target="_blank" rel="noopener">智能单元</a>, 感谢杜客团队的翻译</p>
</blockquote>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/09/07/Backpropagation/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption"><</strong>
      <div class="article-nav-title">
        
          Back Propagation
        
      </div>
    </a>
  
  
    <a href="/2016/09/02/SVM and Softmax/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">SVM and Softmax</div>
      <strong class="article-nav-caption">></strong>
    </a>
  
</nav>

  
</article>










</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2018 simshang
			<a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia Theme</a>
		</div>
      	<div class="footer-right">
			<a href="https://www.google.com/chrome/browser/desktop/index.html" target="_blank">Chrome Recommended </a>
		</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



<div id="totop" style="position:fixed;bottom:85px;right:-5px;cursor: pointer;">
    <a title="返回顶部"><img src="/img/scrollup.png"/></a>
</div>
<script src="/js/totop.js"></script>

  </div>
</body>
</html>